# -*- coding: utf-8 -*-
"""CPCS433_Proj_Group#11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q5ow4vRVN3MppePN2HXLlERyiVjj9o8l
"""

import pandas as pd
import numpy as np
import nltk
nltk.download('stopwords')
import nltk
nltk.download('punkt')
from nltk.tokenize import TweetTokenizer
import re
from nltk.corpus import stopwords 
stopwords_list = stopwords.words('arabic')
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
from nltk.stem.isri import ISRIStemmer
st = ISRIStemmer()
from nltk.tokenize import TweetTokenizer
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.metrics import plot_confusion_matrix
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

#loading dataset from local file use pandas
dataset = pd.read_csv("tweets_dataset.csv")
dataset.info()
print(dataset)

print(dataset.count())
dataset['Label'].value_counts()

#drop "Unrelated"
dataset = dataset.where(dataset.values != 'Unrelated').dropna()      
print("\n\n>>_After Drop *Unrelated*:\n",dataset.count())

#convert categorical values to numerical
change={ "Label":{ "Neutral"  :1,
                    "Negative" :2,
                    "Positive" :3}}
dataset.replace(change,inplace=True)
print(dataset.info())
print(dataset)

# drop duplicates (tweets)
dataset = dataset.drop_duplicates()
print(dataset)

#count after drop Unrelated and duplicates (tweets)
print(dataset.count())
dataset['Label'].value_counts()

#remove noise and normalize
remove = set([
    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',
    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',
    '=-3', '=3', ':-))', ":'-)", ":')", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',
    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',
    '<3',':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',
    ':-[', ':-<', '=\\', '=/', '>:(', ':(', '>.<', ":'-(", ":'(", ':\\', ':-c',
    ':c', ':{', '>:\\', ';(','؟','،','ُ', 'ْ', 'إ', 'ِ', 'ّ', 'َ', 'ً','٪','_','«','أ','آ','ّ','َ','ً','ٌ','ِ','ٍ'
    ])

def clean(text_tweet):
  #remove emojies
  emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642" 
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
        u"\u061c"
        u"\u2067"
                           "]+", flags=re.UNICODE)
  #remove emoji
  text_tweet = emoji_pattern.sub(r'', text_tweet)
  #remove @mention
  text_tweet = re.sub(r'@[^\s]+', '', text_tweet)

	# remove hyperlinks
  text_tweet = re.sub(r'((www\.[^\s]+)|(https?://[^\s]+))', '', text_tweet)
	
	# remove hashtags
	# only removing the hash # and underscore _
  text_tweet = re.sub(r'[#_]+', '', text_tweet)

  #remove repeated letters
  text_tweet = re.sub(r'(.)\1+', r'\1', text_tweet)

  #remove english letter and numbers
  text_tweet = re.sub(r'[a-zA-Z0-9]', '', text_tweet)
  
	# tokenize tweets
  tokenizer_text_tweet = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
  tokens_text_tweet = tokenizer_text_tweet.tokenize(text_tweet)

  clean_text_tweet = []	
  for word in tokens_text_tweet:  
    if (word not in stopwords_list and # remove stopwords  
        word not in remove and # remove
			    word not in string.punctuation): # remove punctuation
      clean_text_tweet.append(word)
  return clean_text_tweet

dataset['Tweet Text'] = dataset['Tweet Text'].apply(clean)
print(dataset['Tweet Text'])
print(dataset)

x = dataset['Tweet Text'].astype(str)
y = dataset['Label']
print(x,"\n\n")
print(y)

# Separating the 60% data for training data and 40% for testing data
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.4, random_state=0)
print('training:',x_train.shape, y_train.shape)
print('testing',x_test.shape, y_test.shape)

############ TfidfVectorizer ############

#(1, 1) means only unigrams 
tfidf_1 = TfidfVectorizer(min_df=0.001,max_df=0.6,lowercase=False,ngram_range=(1,1))
tfidf_1.fit(x_train)
print('No. of feature words: ', len(tfidf_1.get_feature_names()))
print(dataset)

#(1, 2) means unigrams and bigrams
tfidf_2 = TfidfVectorizer(min_df=0.001,max_df=0.6,lowercase=False,ngram_range=(1,2))
tfidf_2.fit(x_train)
print('No. of feature words: ', len(tfidf_2.get_feature_names()))
print(dataset)

#(3,3) trigrams
tfidf_3 = TfidfVectorizer(min_df=0.001,max_df=0.6,lowercase=False,ngram_range=(3,3))
tfidf_3.fit(x_train)
print('No. of feature words: ', len(tfidf_3.get_feature_names()))
print(dataset)

############ Transform ############
#(1, 1) means only unigrams 
x_train_tfidf_1 = tfidf_1.transform(x_train)
x_test_tfidf_1 = tfidf_1.transform(x_test)
print(x_train_tfidf_1)
print(x_test_tfidf_1)

#(1, 2) means unigrams and bigrams
x_train_tfidf_2 = tfidf_2.transform(x_train)
x_test_tfidf_2 = tfidf_2.transform(x_test)
print(x_train_tfidf_2)
print(x_test_tfidf_2)

#(3,3) trigrams
x_train_tfidf_3 = tfidf_3.transform(x_train)
x_test_tfidf_3 = tfidf_3.transform(x_test)
print(x_train_tfidf_3)
print(x_test_tfidf_3)

#Use grid search to find the best hyperparameters for the SVM model
C = [0.1,1,10,100,1000]
gammas=[1,0.1,0.01,0.001,0.0001]
param_grid={'C':C,'gamma':gammas,'kernel':['linear','rbf','poly','sigmoid']}

grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)
grid_search.fit(x_train_tfidf_1,y_train)

#The best hyperparameters for the SVM model
print("The best hyperparameters for the SVM model:\n", grid_search.best_params_)

SVC_Best_tfidf_1=SVC(C=1000, gamma= 0.1, kernel= "rbf")
SVC_Best_tfidf_1.fit(x_train_tfidf_1,y_train)

y_predict_tfidf_1 = SVC_Best_tfidf_1.predict(x_test_tfidf_1)
y_train_predict_tfidf_1 = SVC_Best_tfidf_1.predict(x_train_tfidf_1)

print(classification_report(y_train,y_train_predict_tfidf_1))
print("=================================================================")
print(classification_report(y_test,y_predict_tfidf_1))

print("=================================================================")
print('accuracy',accuracy_score(y_train,y_train_predict_tfidf_1),'\n')
print('precision',precision_score(y_train,y_train_predict_tfidf_1,average='weighted'),'\n')
print('recall',recall_score(y_train,y_train_predict_tfidf_1,average='weighted'),'\n')

print("=================================================================")
print('accuracy',accuracy_score(y_test,y_predict_tfidf_1,),'\n')
print('precision',precision_score(y_test,y_predict_tfidf_1,average='weighted'),'\n')
print('recall',recall_score(y_test,y_predict_tfidf_1,average='weighted'),'\n')

# Compute and plot the Confusion matrix
cf_matrix_tfidf_1 = confusion_matrix(y_test, y_predict_tfidf_1)
print('Confusion Matrix')
print(cf_matrix_tfidf_1,'\n\n' )

ax_tfidf_1 = sns.heatmap(cf_matrix_tfidf_1, annot=False, cmap='Blues')
ax_tfidf_1.set_title('Plot of confusion Matrix\n\n');
ax_tfidf_1.set_xlabel('\nPredicted Values')
ax_tfidf_1.set_ylabel('Actual Values ');

## Display the visualization of the Confusion Matrix.
plt.show()

#Use grid search to find the best hyperparameters for the SVM model
grid_search.fit(x_train_tfidf_2,y_train)
print("The best hyperparameters for the SVM model:\n", grid_search.best_params_)

SVC_Best_tfidf_2=SVC(C=10, gamma= 1, kernel= "linear")
SVC_Best_tfidf_2.fit(x_train_tfidf_2,y_train)

y_predict_tfidf_2 = SVC_Best_tfidf_2.predict(x_test_tfidf_2)
y_train_predict_tfidf_2 = SVC_Best_tfidf_2.predict(x_train_tfidf_2)

print(classification_report(y_train,y_train_predict_tfidf_2))
print("=================================================================")
print(classification_report(y_test,y_predict_tfidf_2))

print("=================================================================")
print('accuracy',accuracy_score(y_train,y_train_predict_tfidf_2),'\n')
print('precision',precision_score(y_train,y_train_predict_tfidf_2,average='weighted'),'\n')
print('recall',recall_score(y_train,y_train_predict_tfidf_2,average='weighted'),'\n')

print("=================================================================")
print('accuracy',accuracy_score(y_test,y_predict_tfidf_2),'\n')
print('precision',precision_score(y_test,y_predict_tfidf_2,average='weighted'),'\n')
print('recall',recall_score(y_test,y_predict_tfidf_2,average='weighted'),'\n')

# Compute and plot the Confusion matrix
cf_matrix_tfidf_2 = confusion_matrix(y_test, y_predict_tfidf_2)
print('Confusion Matrix')
print(cf_matrix_tfidf_2,'\n\n')

ax_tfidf_2 = sns.heatmap(cf_matrix_tfidf_1, annot=False, cmap='Blues')
ax_tfidf_2.set_title('Plot of confusion Matrix\n\n');
ax_tfidf_2.set_xlabel('\nPredicted Values')
ax_tfidf_2.set_ylabel('Actual Values ');

## Display the visualization of the Confusion Matrix.
plt.show()

#Use grid search to find the best hyperparameters for the SVM model
grid_search.fit(x_train_tfidf_3,y_train)
print("The best hyperparameters for the SVM model:\n", grid_search.best_params_)

SVC_Best_tfidf_3=SVC(C=10, gamma= 0.1, kernel= "rbf")
SVC_Best_tfidf_3.fit(x_train_tfidf_3,y_train)

y_predict_tfidf_3 = SVC_Best_tfidf_3.predict(x_test_tfidf_3)
y_train_predict_tfidf_3 = SVC_Best_tfidf_3.predict(x_train_tfidf_3)

print(classification_report(y_train,y_train_predict_tfidf_3))
print("=================================================================")
print(classification_report(y_test,y_predict_tfidf_3))

print("=================================================================")
print('accuracy',accuracy_score(y_train,y_train_predict_tfidf_3),'\n')
print('precision',precision_score(y_train,y_train_predict_tfidf_3,average='weighted'),'\n')
print('recall',recall_score(y_train,y_train_predict_tfidf_3,average='weighted'),'\n')

print("=================================================================")
print('accuracy',accuracy_score(y_test,y_predict_tfidf_3),'\n')
print('precision',precision_score(y_test,y_predict_tfidf_3,average='weighted'),'\n')
print('recall',recall_score(y_test,y_predict_tfidf_3,average='weighted'),'\n')

# Compute and plot the Confusion matrix
cf_matrix_tfidf_3 = confusion_matrix(y_test, y_predict_tfidf_3)
print('Confusion Matrix')
print(cf_matrix_tfidf_3,'\n\n')

ax_tfidf_3 = sns.heatmap(cf_matrix_tfidf_3, annot=False, cmap='Blues')
ax_tfidf_3.set_title('Plot of confusion Matrix\n\n');
ax_tfidf_3.set_xlabel('\nPredicted Values')
ax_tfidf_3.set_ylabel('Actual Values ');

## Display the visualization of the Confusion Matrix.
plt.show()